import { APILink } from "@site/src/components/APILink";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TileCard from "@site/src/components/TileCard";
import TilesGrid from "@site/src/components/TilesGrid";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import ConceptOverview from "@site/src/components/ConceptOverview";
import { MessageSquare, BookOpen, Rocket, Target, FileText, Search, Settings, GitBranch, Users, Zap, Brain, Wrench } from "lucide-react";

# Using make_judge for Custom LLM Evaluation

:::tip[New in MLflow 3.4.0]

The <APILink fn="mlflow.genai.judges.make_judge">make_judge</APILink> API is the new, recommended way to create custom LLM judges in MLflow. It provides a unified, flexible interface for all types of judge-based evaluation.

:::

## Why Use make_judge?

The new `make_judge` API represents a significant advancement in MLflow's evaluation capabilities. It provides a unified, template-based interface that supports both traditional output evaluation and advanced trace analysis, making it the most flexible way to create custom LLM judges.

## Key Features

<FeatureHighlights features={[
  {
    icon: Target,
    title: "Dual Evaluation Modes",
    description: "Evaluate final outputs with field-based assessment or analyze complete execution flows with trace-based evaluation. Perfect for both simple Q&A validation and complex agent debugging."
  },
  {
    icon: FileText,
    title: "Natural Language Templates",
    description: "Write evaluation criteria in plain English using template variable syntax. Variables are automatically extracted, validated, and filled with your data at runtime."
  },
  {
    icon: Wrench,
    title: "Trace Analysis Tools",
    description: "When evaluating traces, judges automatically get specialized tools to inspect spans, search patterns, and analyze execution flows - no manual setup required."
  },
  {
    icon: GitBranch,
    title: "Version Control & Sharing",
    description: "Register judges to Unity Catalog for team collaboration. Track versions, retrieve specific iterations, and share evaluation logic across projects."
  }
]} />

## Evaluation Modes

<ConceptOverview concepts={[
  {
    icon: Brain,
    title: "Field-Based Evaluation",
    description: "Assess specific inputs, outputs, and expectations. Mix variables from different data categories. Ideal for traditional Q&A, classification, and generation tasks where you need to evaluate final results."
  },
  {
    icon: Search,
    title: "Trace-Based Evaluation",
    description: "Analyze complete execution flows using the trace variable. Inspect intermediate steps, tool usage, and decision-making. Essential for debugging complex AI agents and multi-step workflows."
  }
]} />

## Template Variables

The `make_judge` API uses a powerful template system with reserved and custom variables:

<ConceptOverview concepts={[
  {
    icon: Settings,
    title: "Reserved Variables",
    description: "Use 'inputs' for input data, 'outputs' for generated results, 'expectations' for ground truth, and 'trace' for execution flows. Note: trace cannot be mixed with other variables."
  },
  {
    icon: Zap,
    title: "Custom Variables",
    description: "Define any custom variables in your instructions. They're automatically extracted and validated. Perfect for domain-specific criteria and flexible evaluation logic."
  }
]} />

## Quick Start

<Tabs>
  <TabItem value="trace" label="Trace-Based (Recommended)" default>
    ```python
    from mlflow.genai.judges import make_judge

    # Create a judge that analyzes complete execution flows
    trace_judge = make_judge(
        name="agent_performance",
        instructions=(
            "Analyze the {{ trace }} to evaluate the agent's performance.\n\n"
            "Check for:\n"
            "1. Efficient execution and tool usage\n"
            "2. Error handling and recovery\n"
            "3. Logical reasoning flow\n"
            "4. Performance bottlenecks\n\n"
            "Provide a rating: 'excellent', 'good', or 'needs improvement'"
        ),
        model="openai:/gpt-4",  # Note: Cannot use 'databricks' model
    )

    # Evaluate a trace from your agent
    import mlflow

    # Your agent execution (automatically traced)
    # Example: my_agent would be your actual agent instance
    # user_input would be the user's query
    with mlflow.start_span("agent_execution") as span:
        response = "Agent response here"  # my_agent.process_query(user_input)

    # Get and evaluate the trace
    trace = mlflow.get_trace(span.trace_id)
    feedback = trace_judge(trace=trace)
    ```

  </TabItem>
  <TabItem value="field" label="Field-Based">
    ```python
    from mlflow.genai.judges import make_judge

    # Create a judge for evaluating outputs
    quality_judge = make_judge(
        name="response_quality",
        instructions=(
            "Evaluate if {{ response }} answers {{ question }} well.\n"
            "Consider clarity and accuracy.\n"
            "Answer 'excellent', 'good', or 'poor'."
        ),
        model="openai:/gpt-4",
    )

    # Evaluate specific fields
    feedback = quality_judge(
        inputs={"question": "What is MLflow?"},
        outputs={"response": "MLflow is an ML platform."},
    )
    ```

  </TabItem>
</Tabs>

## Important Limitations for Trace Evaluation

:::warning[Parameter Restrictions with Traces]
When using `trace` in your instructions, you **cannot** mix it with other variable types:

- ❌ Cannot use `inputs`, `outputs`, or `expectations` with `trace`
- ❌ Cannot use custom template variables with `trace`
- ❌ Cannot use the `databricks` default model with trace-based judges

The trace contains complete execution context and should not be mixed with partial data.
:::

## Common Evaluation Patterns

<Tabs>
  <TabItem value="trace-patterns" label="Trace Evaluation (Recommended)" default>
    ```python
    # Tool Usage Evaluation
    tool_judge = make_judge(
        name="tool_usage",
        instructions=(
            "Examine the {{ trace }} for tool usage patterns.\n"
            "Check: tool selection, sequencing, output utilization, error handling.\n"
            "Rate as 'optimal', 'acceptable', or 'inefficient'."
        ),
        model="openai:/gpt-4",
    )

    # Reasoning Chain Evaluation
    reasoning_judge = make_judge(
        name="reasoning",
        instructions=(
            "Analyze reasoning in {{ trace }}.\n"
            "Evaluate: logical progression, assumptions, conclusions.\n"
            "Score 0-100 for reasoning quality."
        ),
        model="openai:/gpt-4",
    )

    # Error Recovery Evaluation
    error_judge = make_judge(
        name="error_recovery",
        instructions=(
            "Review {{ trace }} for error handling.\n"
            "Check: detection, recovery strategies, user impact.\n"
            "Rate as 'robust', 'adequate', or 'fragile'."
        ),
        model="openai:/gpt-4",
    )
    ```

  </TabItem>
  <TabItem value="field-patterns" label="Field Evaluation">
    ```python
    # Correctness Evaluation
    correctness_judge = make_judge(
        name="correctness",
        instructions=(
            "Check if {{ response }} correctly answers {{ question }}.\n"
            "Expected: {{ expected_answer }}\n"
            "Answer 'yes' or 'no' with reasoning."
        ),
        model="openai:/gpt-4",
    )

    # Safety Evaluation
    safety_judge = make_judge(
        name="safety",
        instructions=(
            "Check if {{ response }} meets guidelines: {{ guidelines }}\n"
            "Answer 'safe' or 'unsafe' with concerns."
        ),
        model="openai:/gpt-4",
    )

    # Groundedness Evaluation
    grounded_judge = make_judge(
        name="groundedness",
        instructions=(
            "Verify {{ response }} is grounded in {{ context }}.\n"
            "Rate: 'fully', 'mostly', 'partially', or 'not' grounded."
        ),
        model="openai:/gpt-4",
    )
    ```

  </TabItem>
</Tabs>

## Integration with MLflow Evaluation

Judges created with `make_judge` work seamlessly as scorers in MLflow's evaluation framework:

### Using Judges in mlflow.genai.evaluate

```python
from mlflow.genai.judges import make_judge
import mlflow
import pandas as pd

# Create multiple judges for comprehensive evaluation
quality_judge = make_judge(
    name="quality",
    instructions=(
        "Rate the quality of {{ response }} for question {{ question }}. Score 1-5."
    ),
    model="openai:/gpt-4",
)

accuracy_judge = make_judge(
    name="accuracy",
    instructions=(
        "Check if {{ response }} accurately answers {{ question }}.\n"
        "Expected: {{ ground_truth }}\n"
        "Answer 'accurate' or 'inaccurate'."
    ),
    model="openai:/gpt-4",
)

# Prepare evaluation data
eval_data = pd.DataFrame(
    {
        "inputs": [{"question": "What is MLflow?"}],
        "outputs": [
            {"response": "MLflow is an open-source platform for ML lifecycle."}
        ],
        "expectations": [
            {
                "ground_truth": "MLflow is an open-source platform for managing the ML lifecycle."
            }
        ],
    }
)

# Run evaluation with judges as scorers
results = mlflow.genai.evaluate(
    data=eval_data,
    scorers=[quality_judge, accuracy_judge],
)

# Access evaluation results
print(results.metrics)
print(results.tables["eval_results_table"])
```

## Registering and Versioning Judges

Judges can be registered to MLflow experiments for version control and team collaboration:

### Registering a Judge

```python
from mlflow.genai.judges import make_judge
import mlflow

# Set up tracking
mlflow.set_tracking_uri("your-tracking-uri")
experiment_id = mlflow.create_experiment("evaluation-judges")

# Create and register a judge
quality_judge = make_judge(
    name="response_quality",
    instructions=("Evaluate if {{ response }} is high quality for {{ question }}."),
    model="openai:/gpt-4",
)

# Register creates version 1
registered_judge = quality_judge.register(experiment_id)
print(f"Registered as version: {registered_judge.version}")

# Update and register a new version
quality_judge_v2 = make_judge(
    name="response_quality",  # Same name
    instructions=(
        "Evaluate if {{ response }} is high quality, accurate, and complete "
        "for {{ question }}."
    ),
    model="openai:/gpt-4o",  # Updated model
)

# Creates version 2
registered_v2 = quality_judge_v2.register(experiment_id)
```

### Retrieving Registered Judges

```python
from mlflow.genai.scorers import get_scorer, list_scorers

# Get the latest version
latest_judge = get_scorer(name="response_quality", experiment_id=experiment_id)

# Get a specific version
v1_judge = get_scorer(name="response_quality", experiment_id=experiment_id, version=1)

# List all judges in an experiment
all_judges = list_scorers(experiment_id=experiment_id)
for judge in all_judges:
    print(f"Judge: {judge.name}, Model: {judge.model}")
```

## Migrating from Legacy Judges

If you're using the older judge functions (`is_correct`, `is_grounded`, etc.), the new `make_judge` API offers a more flexible and powerful alternative:

### Key Advantages of Migration

1. **Unified API**: One function for all judge types instead of multiple specialized functions
2. **Custom Variables**: Define any template variables you need
3. **Better Integration**: Works seamlessly as a scorer in evaluation
4. **Version Control**: Register and version judges for reproducibility

### Migration Example

<Tabs>
  <TabItem value="legacy" label="Legacy Approach" default>
    ```python
    from mlflow.genai.judges import is_correct

    # Limited to predefined parameters
    feedback = is_correct(
        request="What is 2+2?", response="4", expected_response="4", model="openai:/gpt-4"
    )
    ```

  </TabItem>
  <TabItem value="new" label="New Approach">
    ```python
    from mlflow.genai.judges import make_judge

    # Flexible template-based approach
    correctness_judge = make_judge(
        name="correctness",
        instructions=(
            "Evaluate if {{ response }} correctly answers {{ question }}.\n"
            "Expected: {{ expected_answer }}\n\n"
            "Consider partial credit for reasoning.\n"
            "Answer: 'correct', 'partially correct', or 'incorrect'"
        ),
        model="openai:/gpt-4",
    )

    feedback = correctness_judge(
        inputs={"question": "What is 2+2?"},
        outputs={"response": "4"},
        expectations={"expected_answer": "4"},
    )
    ```

  </TabItem>
</Tabs>

## Advanced Features

### Working with Complex Data

```python
# Judge that handles multiple data types
comprehensive_judge = make_judge(
    name="comprehensive_eval",
    instructions=(
        "Evaluate the complete interaction:\n\n"
        "User Profile: {{ user_profile }}\n"
        "Query: {{ query }}\n"
        "Context Documents: {{ context }}\n"
        "Model Response: {{ response }}\n"
        "Expected Topics: {{ required_topics }}\n\n"
        "Assess completeness, accuracy, and appropriateness."
    ),
    model="openai:/gpt-4",
)

# Handle complex nested data
feedback = comprehensive_judge(
    inputs={
        "user_profile": {"expertise": "beginner", "domain": "ML"},
        "query": "Explain neural networks",
        "context": ["Document 1...", "Document 2..."],
    },
    outputs={"response": "Neural networks are..."},
    expectations={"required_topics": ["layers", "neurons", "activation functions"]},
)
```

### Conditional Logic in Instructions

```python
conditional_judge = make_judge(
    name="adaptive_evaluator",
    instructions=(
        "Evaluate {{ response }} based on the user level {{ user_level }}:\n\n"
        "If user_level is 'beginner':\n"
        "- Check for simple language\n"
        "- Ensure no unexplained jargon\n\n"
        "If user_level is 'expert':\n"
        "- Check for technical accuracy\n"
        "- Ensure appropriate depth\n\n"
        "Rate as 'appropriate' or 'inappropriate' for the user level."
    ),
    model="openai:/gpt-4",
)
```

## Advanced Trace Workflows

### Trace Analysis Tools

When evaluating traces, judges automatically receive specialized tools:

- `get_trace_info` - Retrieve metadata and timing
- `list_spans` - View all execution steps
- `search_trace_regex` - Find patterns across traces
- `get_span` - Inspect specific steps
- `get_root_span` - Access top-level context

### Complete Workflow Example

```python
import mlflow
from mlflow.genai.judges import make_judge

# Create a performance judge
perf_judge = make_judge(
    name="performance",
    instructions=(
        "Analyze {{ trace }} for: slow operations (>2s), redundancy, efficiency.\n"
        "Rate: 'fast', 'acceptable', or 'slow'. List bottlenecks."
    ),
    model="openai:/gpt-4",
)

# Collect and evaluate traces
traces = []
for query in ["What is MLflow?", "How to track?"]:
    with mlflow.start_span("query") as span:
        # Example: my_agent would be your actual agent instance
        response = "Response for: " + query  # my_agent.process(query)
        trace = mlflow.get_trace(span.trace_id)

        # Evaluate immediately
        feedback = perf_judge(trace=trace)
        traces.append(
            {"query": query, "rating": feedback.value, "issues": feedback.rationale}
        )

# Log evaluation results
with mlflow.start_run():
    mlflow.log_table(traces, "trace_evaluations.json")
```

### Combining with Human Feedback

Automate initial analysis and flag traces for human review:

```python
# Automated evaluation
auto_feedback = trace_quality_judge(trace=trace)

# Flag for human review if needed
if auto_feedback.value in ["poor", "needs improvement"]:
    # Example: human_review_queue would be your review system
    # human_review_queue.add({...})
    review_item = {
        "trace_id": trace.trace_id,
        "auto_rating": auto_feedback.value,
        "needs_review": True,
    }

# Combine automated and human insights
final_assessment = {
    "trace_id": trace.trace_id,
    "automated": auto_feedback,
    # "human": await get_human_feedback(trace.trace_id),
}
```

## Learn More

<TilesGrid>
  <TileCard
    icon={MessageSquare}
    iconSize={48}
    title="Evaluation Quickstart"
    description="Get started with MLflow's evaluation framework and learn best practices."
    href="/genai/eval-monitor/quickstart"
    linkText="Start evaluating →"
    containerHeight={64}
  />
  <TileCard
    icon={BookOpen}
    iconSize={48}
    title="Predefined Judges"
    description="Explore MLflow's built-in LLM judges for common evaluation tasks."
    href="/genai/eval-monitor/scorers/llm-judge/predefined"
    linkText="View built-in judges →"
    containerHeight={64}
  />
  <TileCard
    icon={Rocket}
    iconSize={48}
    title="Human Feedback"
    description="Learn how to collect and utilize human feedback alongside LLM judges."
    href="/genai/tracing/collect-user-feedback"
    linkText="Collect feedback →"
    containerHeight={64}
  />
  <TileCard
    icon={GitBranch}
    iconSize={48}
    title="Tracing Guide"
    description="Learn how to collect and analyze traces for comprehensive evaluation."
    href="/genai/tracing"
    linkText="Start tracing →"
    containerHeight={64}
  />
</TilesGrid>
